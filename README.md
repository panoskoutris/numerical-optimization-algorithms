# Numerical Optimization Algorithms

MATLAB implementations of numerical optimization algorithms, covering univariate, multivariate, and constrained optimization. Each part focuses on applying specific methods to minimize given mathematical functions under various conditions and parameters.

Developed as part of the course Optimization Techniques at the Aristotle University of Thessaloniki (AUTH), School of Electrical & Computer Engineering.

## ğŸ“˜ Part 1: Univariate Optimization

**Goal:** Minimize three convex one-dimensional functions over fixed intervals using derivative-free and derivative-based methods.

### ğŸ” Methods Implemented:

- **Bisection Method** (without derivative)
- **Bisection Method with derivative**
- **Golden Section Search**
- **Fibonacci Search**

### ğŸ§ª Applied To:
Each method is tested on 3 different convex functions defined on a closed interval \([a, b]\), with varying:
- Interval tolerance `l`
- Accuracy parameter `Îµ`

### ğŸ”§ Tasks:
- Compare performance by iteration count and function evaluations
- Plot convergence behavior across all methods
- Analyze robustness and sensitivity to parameter changes

---

## ğŸ“˜ Part 2: Multivariate Optimization

**Goal:** Minimize the two-variable function:
\[
f(x, y) = x^5 \cdot e^{-(x^2 + y^2)}
\]

### ğŸ” Methods Implemented:

- **Steepest Descent** (Gradient Descent)
- **Newton's Method**
- **Levenbergâ€“Marquardt Algorithm**

### ğŸš€ Initialization:
Each method is tested from the following starting points:
- (0, 0)
- (âˆ’1, 1)
- (1, âˆ’1)

### ğŸ§­ Step Size Strategies:
- Fixed step size
- **Exact Line Search** using Bisection
- **Armijo Rule**

### ğŸ”§ Tasks:
- Evaluate convergence for each methodâ€“pointâ€“strategy combination
- Analyze convergence speed, stability, and path taken
- Visualize and interpret convergence surfaces and trajectories

---

## ğŸ“˜ Part 3: Constrained Optimization

**Goal:** Minimize a quadratic function with projection to satisfy box constraints.

### ğŸ“‰ Function:
\[
f(x_1, x_2) = \frac{1}{3}x_1^2 + 3x_2^2
\]

### ğŸ“ Constraints (Box):
- \(-10 \leq x_1 \leq 5\)
- \(0 \leq x_2 \leq 10\)

### ğŸ” Method Implemented:

- **Projected Gradient Descent**
  - Uses projection operator \( \text{Pr}_X \{\cdot\} \) to ensure feasibility

### ğŸ§ª Experiments:
- Run the algorithm with different step sizes `s` and interpolation factors `Î³`
- Apply method from different starting points: e.g. (4,5), (5, -5), (âˆ’5, 10)
- Assess convergence, feasibility, and oscillatory behavior

### ğŸ§  Key Insight:
Stability and convergence depend heavily on the product \( s \cdot \gamma \) staying below a derived theoretical threshold (e.g. \( < \frac{1}{3} \))

---

## ğŸ“Š Tools and Languages

- MATLAB (R2021b or newer recommended)
- Plotting utilities for convergence and trajectory visualization

## ğŸš€ How to Run

TO BE IMPLEMENTED

## ğŸ“ Repository Structure

```
numerical-optimization-algorithms/
â”œâ”€â”€ part1-univariate-methods/
â”‚ â”œâ”€â”€ bisection.m
â”‚ â”œâ”€â”€ bisection_with_derivative.m
â”‚ â”œâ”€â”€ golden_section.m
â”‚ â”œâ”€â”€ fibonacci.m
â”‚ â””â”€â”€ plots/ # Convergence plots for different l and Îµ
â”‚
â”œâ”€â”€ part2-multivariate-methods/
â”‚ â”œâ”€â”€ steepest_descent.m
â”‚ â”œâ”€â”€ newton_method.m
â”‚ â”œâ”€â”€ levenberg_marquardt.m
â”‚ â”œâ”€â”€ line_search_strategies.m
â”‚ â””â”€â”€ plots/ # Convergence behavior for different step rules and starting points
â”‚
â”œâ”€â”€ part3-projection-method/
â”‚ â”œâ”€â”€ projected_gradient_descent.m
â”‚ â”œâ”€â”€ constraints.m
â”‚ â””â”€â”€ plots/ # Iteration paths and convergence under box constraints
â”‚
â””â”€â”€ README.md
```
## âœï¸ Author

**Panagiotis Koutris**  
Student at ECE AUTH â€“ School of Electrical & Computer Engineering


## ğŸ“„ License

This project is released under the MIT License.

